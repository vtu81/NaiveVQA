{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# model (Mindspore implementation)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import nn, Tensor\n",
    "from mindspore import ops as P\n",
    "import mindspore.common.initializer as init\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import config\n",
    "import mindspore.context as context\n",
    "\n",
    "context.set_context(device_target=\"GPU\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型定义\n",
    "\n",
    "核心模型由以下三部分构成：\n",
    "* LSTM提取文本特征 `TextProcessor`\n",
    "* SAN注意力机制 `Attention`\n",
    "* 分类器 `Classifier`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `TextProcessor`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* some initializations are removed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "class TextProcessor(nn.Cell):\n",
    "    def __init__(self, embedding_tokens, embedding_features, lstm_features, drop=0.0):\n",
    "        super(TextProcessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(embedding_tokens, embedding_features, padding_idx=0)\n",
    "        self.drop = nn.Dropout(keep_prob=1 - drop)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(input_size=embedding_features,\n",
    "                            hidden_size=lstm_features,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "        self.features = lstm_features\n",
    "    def construct(self, q, q_len):\n",
    "        embedded = self.embedding(q)\n",
    "        tanhed = self.tanh(self.drop(embedded))\n",
    "\n",
    "        h0 = Tensor(np.ones([1, q.shape[0], self.features]).astype(np.float32))\n",
    "        c0 = Tensor(np.ones([1, q.shape[0], self.features]).astype(np.float32))\n",
    "        \n",
    "        _, (_, c) = self.lstm(tanhed, (h0, c0))\n",
    "\n",
    "        return c.squeeze(0) # only supported from 1.2.x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "test:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "net = TextProcessor(5201, 300, 1024) # vocab size = 5201, embedding size = 300, hidden size of LSTM = 1024\n",
    "q = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=mindspore.int32) # batch size of 3, each sentence contains 3 tokens\n",
    "q_len = Tensor([[3, 3, 3]])\n",
    "output = net(q, q_len)\n",
    "print(output.shape) # 3 extracted features"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 1024)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `Attention`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def tile_2d_over_nd(feature_vector, feature_map):\n",
    "    \"\"\" Repeat the same feature vector over all spatial positions of a given feature map.\n",
    "        The feature vector should have the same batch size and number of features as the feature map.\n",
    "    \"\"\"\n",
    "    n, c = feature_vector.shape\n",
    "    spatial_size = feature_map.dim() - 2\n",
    "    tiled = feature_vector.view(n, c, *([1] * spatial_size)).expand_as(feature_map)\n",
    "    return tiled\n",
    "\n",
    "class Attention(nn.Cell):\n",
    "    def __init__(self, v_features, q_features, mid_features, glimpses, drop=0.0):\n",
    "        super(Attention, self).__init__()\n",
    "        self.v_conv = nn.Conv2d(v_features, mid_features, 1, has_bias=False)  # let self.lin take care of bias\n",
    "        self.q_lin = nn.Dense(q_features, mid_features)\n",
    "        self.x_conv = nn.Conv2d(mid_features, glimpses, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(1 - drop)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, v, q):\n",
    "        v = self.v_conv(self.drop(v))\n",
    "        q = self.q_lin(self.drop(q))\n",
    "        q = tile_2d_over_nd(q, v)\n",
    "        x = self.relu(v + q)\n",
    "        x = self.x_conv(self.drop(x))\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "test:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "net = Attention(3, 3, 512, 2, 0.5)\n",
    "v = Tensor(np.ones([3, 3, 14, 14]), dtype=mindspore.float32) # batch size of 3, shape of 3 * 3 * 14 * 14\n",
    "q = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=mindspore.float32) # batch size of 3, shape of 3 * 3\n",
    "print(net(v, q).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 2, 14, 14)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `Classifier`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class Classifier(nn.SequentialCell):\n",
    "    def __init__(self, in_features, mid_features, out_features, drop=0.0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.insert_child_to_cell('drop1', nn.Dropout(keep_prob=1 - drop))\n",
    "        self.insert_child_to_cell('lin1', nn.Dense(in_features, mid_features))\n",
    "        self.insert_child_to_cell('relu', nn.ReLU())\n",
    "        self.insert_child_to_cell('drop2', nn.Dropout(keep_prob=1 - drop))\n",
    "        self.insert_child_to_cell('lin2', nn.Dense(mid_features, out_features))\n",
    "        self.cell_list = list(self._cells.values())\n",
    "\n",
    "# class Classifier(nn.Cell):\n",
    "    # def __init__(self, in_features, mid_features, out_features, drop=0.0):\n",
    "    #     super(Classifier, self).__init__()\n",
    "    #     self.insert_child_to_cell('drop1', nn.Dropout(keep_prob=1 - drop))\n",
    "    #     self.insert_child_to_cell('lin1', nn.Dense(in_features, mid_features))\n",
    "    #     self.insert_child_to_cell('relu', nn.ReLU())\n",
    "    #     self.insert_child_to_cell('drop2', nn.Dropout(keep_prob=1 - drop))\n",
    "    #     self.insert_child_to_cell('lin2', nn.Dense(mid_features, out_features))\n",
    "    # def construct(self, inputs):\n",
    "    #     drop1 = self.drop1(inputs)\n",
    "    #     lin1 = self.lin1(drop1)\n",
    "    #     lin1 = self.relu(lin1)\n",
    "    #     drop2 = self.drop2(lin1)\n",
    "    #     lin2 = self.lin2(drop2)\n",
    "    #     return lin2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "test:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "net = Classifier(3, 4, 5)\n",
    "x = mindspore.Tensor([[1, 2, 3]], dtype=mindspore.float32)\n",
    "print(\"Operations\", net.cell_list)\n",
    "print(net(x).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Operations [Dropout<keep_prob=1.0>, Dense<input_channels=3, output_channels=4, has_bias=True>, ReLU<>, Dropout<keep_prob=1.0>, Dense<input_channels=4, output_channels=5, has_bias=True>]\n",
      "(1, 5)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `Net`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "def apply_attention(input, attention):\n",
    "    \"\"\" Apply any number of attention maps over the input. \"\"\"\n",
    "    softmax = P.Softmax(axis=-1)\n",
    "    unsqueeze = P.ExpandDims()\n",
    "    reduce_sum = P.ReduceSum()\n",
    "\n",
    "    n, c = input.shape[:2]\n",
    "    glimpses = attention.shape[1]\n",
    "\n",
    "    # flatten the spatial dims into the third dim, since we don't need to care about how they are arranged\n",
    "    input = input.view(n, 1, c, -1) # [n, 1, c, s]\n",
    "    attention = attention.view(n, glimpses, -1)\n",
    "    attention = unsqueeze(softmax(attention), 2) # [n, g, 1, s]\n",
    "    weighted = attention * input # [n, g, v, s]\n",
    "    weighted_mean = reduce_sum(weighted, -1) # [n, g, v]\n",
    "    return weighted_mean.view(n, -1)\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    \"\"\" Re-implementation of ``Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'' [0]\n",
    "\n",
    "    [0]: https://arxiv.org/abs/1704.03162\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_tokens):\n",
    "        super(Net, self).__init__()\n",
    "        question_features = 1024\n",
    "        vision_features = config.output_features\n",
    "        glimpses = 2\n",
    "\n",
    "        self.text = TextProcessor(\n",
    "            embedding_tokens=embedding_tokens,\n",
    "            embedding_features=300,\n",
    "            lstm_features=question_features,\n",
    "            drop=0.5,\n",
    "        )\n",
    "        self.attention = Attention(\n",
    "            v_features=vision_features,\n",
    "            q_features=question_features,\n",
    "            mid_features=512,\n",
    "            glimpses=2,\n",
    "            drop=0.5,\n",
    "        )\n",
    "        self.classifier = Classifier(\n",
    "            in_features=glimpses * vision_features + question_features,\n",
    "            mid_features=1024,\n",
    "            out_features=config.max_answers,\n",
    "            drop=0.5,\n",
    "        )\n",
    "        self.cat = P.Concat(axis=1)\n",
    "        self.norm = nn.Norm(axis=1, keep_dims=True)\n",
    "\n",
    "    def construct(self, v, q, q_len):\n",
    "        q = self.text(q, q_len)\n",
    "\n",
    "        v = v / (self.norm(v).expand_as(v) + 1e-8)\n",
    "        a = self.attention(v, q)\n",
    "        v = apply_attention(v, a)\n",
    "        combined = self.cat((v, q))\n",
    "        print(combined.shape)\n",
    "        answer = self.classifier(combined)\n",
    "        return answer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "test:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "net = Net(5201)\n",
    "v = Tensor(np.ones([3, 2048, 14, 14]), dtype=mindspore.float32) # batch size of 3, shape of 3 * 3 * 14 * 14\n",
    "q = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=mindspore.int32) # batch size of 3, each sentence contains 3 tokens\n",
    "q_len = Tensor([[3, 3, 3]])\n",
    "print(net(v, q, q_len).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 5120)\n",
      "(3, 350)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}