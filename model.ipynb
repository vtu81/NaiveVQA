{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# model (Mindspore implementation)"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import numpy as np\nimport mindspore\nfrom mindspore import nn, Tensor\nfrom mindspore import ops as P\nimport mindspore.common.initializer as init\n# from torch.nn.utils.rnn import pack_padded_sequence\nimport config\nimport mindspore.context as context\n\ncontext.set_context(device_target=\"CPU\")", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## \u6a21\u578b\u5b9a\u4e49\n\n\u6838\u5fc3\u6a21\u578b\u7531\u4ee5\u4e0b\u4e09\u90e8\u5206\u6784\u6210\uff1a\n* LSTM\u63d0\u53d6\u6587\u672c\u7279\u5f81 `TextProcessor`\n* SAN\u6ce8\u610f\u529b\u673a\u5236 `Attention`\n* \u5206\u7c7b\u5668 `Classifier`"}, {"metadata": {}, "cell_type": "markdown", "source": "### `TextProcessor`"}, {"metadata": {}, "cell_type": "markdown", "source": "* some initializations are removed"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "class TextProcessor(nn.Cell):\n    def __init__(self, embedding_tokens, embedding_features, lstm_features, drop=0.0):\n        super(TextProcessor, self).__init__()\n        self.embedding = nn.Embedding(embedding_tokens, embedding_features, padding_idx=0)\n        self.drop = nn.Dropout(keep_prob=1 - drop)\n        self.tanh = nn.Tanh()\n        self.lstm = nn.LSTM(input_size=embedding_features,\n                            hidden_size=lstm_features,\n                            num_layers=1,\n                            batch_first=True)\n        self.features = lstm_features\n    def construct(self, q, q_len):\n        embedded = self.embedding(q)\n        tanhed = self.tanh(self.drop(embedded))\n\n        h0 = Tensor(np.ones([1, q.shape[0], self.features]).astype(np.float32))\n        c0 = Tensor(np.ones([1, q.shape[0], self.features]).astype(np.float32))\n        \n        _, (_, c) = self.lstm(tanhed, (h0, c0))\n\n        return c.squeeze(0) # only supported from 1.2.x", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "test:"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "net = TextProcessor(5201, 300, 1024) # vocab size = 5201, embedding size = 300, hidden size of LSTM = 1024\nq = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=mindspore.int32) # batch size of 3, each sentence contains 3 tokens\nq_len = Tensor([[3, 3, 3]])\noutput = net(q, q_len)\nprint(output.shape) # 3 extracted features", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "(3, 1024)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### `Attention`"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def tile_2d_over_nd(feature_vector, feature_map):\n    \"\"\" Repeat the same feature vector over all spatial positions of a given feature map.\n        The feature vector should have the same batch size and number of features as the feature map.\n    \"\"\"\n    n, c = feature_vector.shape\n    spatial_size = feature_map.dim() - 2\n    tiled = feature_vector.view(n, c, *([1] * spatial_size)).expand_as(feature_map)\n    return tiled\n\nclass Attention(nn.Cell):\n    def __init__(self, v_features, q_features, mid_features, glimpses, drop=0.0):\n        super(Attention, self).__init__()\n        self.v_conv = nn.Conv2d(v_features, mid_features, 1, has_bias=False)  # let self.lin take care of bias\n        self.q_lin = nn.Dense(q_features, mid_features)\n        self.x_conv = nn.Conv2d(mid_features, glimpses, 1)\n\n        self.drop = nn.Dropout(1 - drop)\n        self.relu = nn.ReLU()\n\n    def construct(self, v, q):\n        v = self.v_conv(self.drop(v))\n        q = self.q_lin(self.drop(q))\n        q = tile_2d_over_nd(q, v)\n        x = self.relu(v + q)\n        x = self.x_conv(self.drop(x))\n        return x", "execution_count": 9, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(mindspore.__version__)", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "1.2.0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "test:"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "net = Attention(3, 3, 512, 2, 0.5)\nv = Tensor(np.ones([3, 3, 14, 14]), dtype=mindspore.float32) # batch size of 3, shape of 3 * 3 * 14 * 14\nq = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=mindspore.float32) # batch size of 3, shape of 3 * 3\nprint(net(v, q).shape)", "execution_count": 10, "outputs": [{"output_type": "error", "ename": "RuntimeError", "evalue": "mindspore/ccsrc/runtime/device/cpu/kernel_select_cpu.cc:299 SetKernelInfo] Operator[BroadcastTo] is not support. Trace: ", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)", "\u001b[0;32m<ipython-input-10-8c93578e5ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmindspore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch size of 3, shape of 3 * 3 * 14 * 14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmindspore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch size of 3, shape of 3 * 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/nn/cell.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcast_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/nn/cell.py\u001b[0m in \u001b[0;36mrun_construct\u001b[0;34m(self, cast_inputs, kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0m_pynative_exec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m             \u001b[0m_pynative_exec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleave_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-9-87c1b82d159e>\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self, v, q)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtile_2d_over_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-9-87c1b82d159e>\u001b[0m in \u001b[0;36mtile_2d_over_nd\u001b[0;34m(feature_vector, feature_map)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mspatial_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtiled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mspatial_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/common/tensor.py\u001b[0m in \u001b[0;36mexpand_as\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_operator_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'broadcast_to'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/ops/primitive.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_elim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/common/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_convert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m~/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages/mindspore/ops/primitive.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(obj, op_name, args)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;34m\"\"\"Single op execution function supported by ge in PyNative mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mRuntimeError\u001b[0m: mindspore/ccsrc/runtime/device/cpu/kernel_select_cpu.cc:299 SetKernelInfo] Operator[BroadcastTo] is not support. Trace: "]}]}, {"metadata": {}, "cell_type": "markdown", "source": "### `Classifier`"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "class Classifier(nn.SequentialCell):\n    def __init__(self, in_features, mid_features, out_features, drop=0.0):\n        super(Classifier, self).__init__()\n        self.insert_child_to_cell('drop1', nn.Dropout(keep_prob=1 - drop))\n        self.insert_child_to_cell('lin1', nn.Dense(in_features, mid_features))\n        self.insert_child_to_cell('relu', nn.ReLU())\n        self.insert_child_to_cell('drop2', nn.Dropout(keep_prob=1 - drop))\n        self.insert_child_to_cell('lin2', nn.Dense(mid_features, out_features))\n        self.cell_list = list(self._cells.values())\n\n# class Classifier(nn.Cell):\n    # def __init__(self, in_features, mid_features, out_features, drop=0.0):\n    #     super(Classifier, self).__init__()\n    #     self.insert_child_to_cell('drop1', nn.Dropout(keep_prob=1 - drop))\n    #     self.insert_child_to_cell('lin1', nn.Dense(in_features, mid_features))\n    #     self.insert_child_to_cell('relu', nn.ReLU())\n    #     self.insert_child_to_cell('drop2', nn.Dropout(keep_prob=1 - drop))\n    #     self.insert_child_to_cell('lin2', nn.Dense(mid_features, out_features))\n    # def construct(self, inputs):\n    #     drop1 = self.drop1(inputs)\n    #     lin1 = self.lin1(drop1)\n    #     lin1 = self.relu(lin1)\n    #     drop2 = self.drop2(lin1)\n    #     lin2 = self.lin2(drop2)\n    #     return lin2", "execution_count": 30, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "test:"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "net = Classifier(3, 4, 5)\nx = mindspore.Tensor([[1, 2, 3]], dtype=mindspore.float32)\nprint(\"Operations\", net.cell_list)\nprint(net(x).shape)", "execution_count": 31, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Operations [Dropout<keep_prob=1.0>, Dense<input_channels=3, output_channels=4, has_bias=True>, ReLU<>, Dropout<keep_prob=1.0>, Dense<input_channels=4, output_channels=5, has_bias=True>]\n(1, 5)\n"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### `Net`"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "def apply_attention(input, attention):\n    \"\"\" Apply any number of attention maps over the input. \"\"\"\n    softmax = P.Softmax(axis=-1)\n    unsqueeze = P.ExpandDims()\n    reduce_sum = P.ReduceSum()\n\n    n, c = input.shape[:2]\n    glimpses = attention.shape[1]\n\n    # flatten the spatial dims into the third dim, since we don't need to care about how they are arranged\n    input = input.view(n, 1, c, -1) # [n, 1, c, s]\n    attention = attention.view(n, glimpses, -1)\n    attention = unsqueeze(softmax(attention), 2) # [n, g, 1, s]\n    weighted = attention * input # [n, g, v, s]\n    weighted_mean = reduce_sum(weighted, -1) # [n, g, v]\n    return weighted_mean.view(n, -1)\n\nclass Net(nn.Cell):\n    \"\"\" Re-implementation of ``Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'' [0]\n\n    [0]: https://arxiv.org/abs/1704.03162\n    \"\"\"\n\n    def __init__(self, embedding_tokens):\n        super(Net, self).__init__()\n        question_features = 1024\n        vision_features = config.output_features\n        glimpses = 2\n\n        self.text = TextProcessor(\n            embedding_tokens=embedding_tokens,\n            embedding_features=300,\n            lstm_features=question_features,\n            drop=0.5,\n        )\n        self.attention = Attention(\n            v_features=vision_features,\n            q_features=question_features,\n            mid_features=512,\n            glimpses=2,\n            drop=0.5,\n        )\n        self.classifier = Classifier(\n            in_features=glimpses * vision_features + question_features,\n            mid_features=1024,\n            out_features=config.max_answers,\n            drop=0.5,\n        )\n        self.cat = P.Concat(axis=1)\n        self.norm = nn.Norm(axis=1, keep_dims=True)\n\n    def construct(self, v, q, q_len):\n        q = self.text(q, q_len)\n\n        v = v / (self.norm(v).expand_as(v) + 1e-8)\n        a = self.attention(v, q)\n        v = apply_attention(v, a)\n        combined = self.cat((v, q))\n        print(combined.shape)\n        answer = self.classifier(combined)\n        return answer", "execution_count": 71, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "test:"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "net = Net(5201)\nv = Tensor(np.ones([3, 2048, 14, 14]), dtype=mindspore.float32) # batch size of 3, shape of 3 * 3 * 14 * 14\nq = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=mindspore.int32) # batch size of 3, each sentence contains 3 tokens\nq_len = Tensor([[3, 3, 3]])\nprint(net(v, q, q_len).shape)", "execution_count": 72, "outputs": [{"output_type": "stream", "name": "stdout", "text": "(3, 5120)\n(3, 350)\n"}]}], "metadata": {"language_info": {"name": "python", "version": "3.7.6", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "mindspore-python3.7-aarch64", "display_name": "MindSpore-python3.7-aarch64", "language": "python"}}, "nbformat": 4, "nbformat_minor": 2}