{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# train (Mindspore implementation)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "import os.path\n",
    "import mindspore\n",
    "from mindspore import Tensor, nn, Model, context\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore import ops as P\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import composite as C\n",
    "from mindspore.common.parameter import ParameterTuple\n",
    "from mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\n",
    "from mindspore.nn.loss.loss import _Loss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import config\n",
    "import data\n",
    "import model\n",
    "import utils\n",
    "import mindspore.context as context\n",
    "\n",
    "# context.set_context(mode=context.PYNATIVE_MODE, device_target='Ascend')\n",
    "context.set_context(mode=context.PYNATIVE_MODE, device_target='GPU')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 1\n",
    "\n",
    "* Not recommended! Please use option 2 :(((("
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class NLLLoss(_Loss):\n",
    "    '''\n",
    "       NLLLoss function\n",
    "    '''\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(NLLLoss, self).__init__(reduction)\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "        self.log_softmax = P.LogSoftmax(axis=0)\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        nll = -self.log_softmax(logits)\n",
    "        loss = self.reduce_sum(nll * label / 10, axis=1).mean()\n",
    "        return self.get_loss(loss)\n",
    "\n",
    "class WithLossCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    The cell wrapped with NLL loss, for train only\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._loss_fn = NLLLoss()\n",
    "        self._backbone = backbone\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "\n",
    "    def construct(self, v, q, a, item, q_len):\n",
    "        out = self._backbone(v, q, q_len)\n",
    "        loss = self._loss_fn(out, a)\n",
    "        return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "test:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "train_loader = data.get_loader(train=True)\n",
    "net = model.Net(train_loader.source.num_tokens)\n",
    "net = WithLossCell(net)\n",
    "for v, q, a, idx, q_len in train_loader:\n",
    "    print(v.shape, q.shape, a.shape)\n",
    "    print(a.shape)\n",
    "    out = net(v, q, a, idx, q_len)\n",
    "    print(out)\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.067258\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train now!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from datetime import datetime\n",
    "name = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "target_name = os.path.join('logs', '{}.ckpt'.format(name))\n",
    "print('will save to {}'.format(target_name))\n",
    "config_as_dict = {k: v for k, v in vars(config).items() if not k.startswith('__')}\n",
    "\n",
    "train_loader = data.get_loader(train=True)\n",
    "# val_loader = data.get_loader(val=True)\n",
    "\n",
    "net = model.Net(train_loader.source.num_tokens)\n",
    "if config.pretrained:\n",
    "    param_dict = load_checkpoint(config.pretrained_model_path)\n",
    "    if param_dict is not None: print(\"Successfully loaded pretrained model from {}.\".format(config.pretrained_model_path))\n",
    "    load_param_into_net(net, param_dict)\n",
    "\n",
    "loss_net = WithLossCell(net) # self defined WithLossCell\n",
    "optimizer = nn.Adam(params=net.trainable_params(), learning_rate=config.initial_lr)\n",
    "train_net = Model(network=loss_net, optimizer=optimizer) # wrap in model\n",
    "\n",
    "loss_cb = LossMonitor()\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=150, keep_checkpoint_max=1)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"ms\", directory='logs', config=config_ck)\n",
    "time_cb = TimeMonitor()\n",
    "callbacks = [time_cb, ckpoint_cb, loss_cb]\n",
    "\n",
    "train_net.train(epoch=config.epochs, train_dataset=train_loader, callbacks=callbacks, dataset_sink_mode=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "will save to logs/2021-07-15_11:45:01.ckpt\n",
      "epoch: 1 step: 1, loss is 2.7623253\n",
      "epoch: 1 step: 2, loss is 2.8053799\n",
      "epoch: 1 step: 3, loss is 2.881455\n",
      "epoch: 1 step: 4, loss is 3.04573\n",
      "epoch: 1 step: 5, loss is 2.6881871\n",
      "epoch: 1 step: 6, loss is 2.8143573\n",
      "epoch: 1 step: 7, loss is 2.986301\n",
      "epoch: 1 step: 8, loss is 3.119298\n",
      "epoch: 1 step: 9, loss is 2.9587572\n",
      "epoch: 1 step: 10, loss is 2.702518\n",
      "epoch: 1 step: 11, loss is 2.7756617\n",
      "epoch: 1 step: 12, loss is 2.8608723\n",
      "epoch: 1 step: 13, loss is 2.9280546\n",
      "epoch: 1 step: 14, loss is 2.6857605\n",
      "epoch: 1 step: 15, loss is 2.8104558\n",
      "epoch: 1 step: 16, loss is 2.4992619\n",
      "epoch: 1 step: 17, loss is 2.7337642\n",
      "epoch: 1 step: 18, loss is 3.0796766\n",
      "epoch: 1 step: 19, loss is 3.15968\n",
      "epoch: 1 step: 20, loss is 2.9457896\n",
      "epoch: 1 step: 21, loss is 3.0661964\n",
      "epoch: 1 step: 22, loss is 2.6680708\n",
      "epoch: 1 step: 23, loss is 2.9891412\n",
      "epoch: 1 step: 24, loss is 2.6883287\n",
      "epoch: 1 step: 25, loss is 3.0324857\n",
      "epoch: 1 step: 26, loss is 3.0038111\n",
      "epoch: 1 step: 27, loss is 3.1528132\n",
      "epoch: 1 step: 28, loss is 2.808564\n",
      "epoch: 1 step: 29, loss is 2.7730753\n",
      "epoch: 1 step: 30, loss is 2.998032\n",
      "epoch: 1 step: 31, loss is 2.794858\n",
      "epoch: 1 step: 32, loss is 2.8652997\n",
      "epoch: 1 step: 33, loss is 3.029668\n",
      "epoch: 1 step: 34, loss is 2.8601944\n",
      "epoch: 1 step: 35, loss is 2.8279805\n",
      "epoch: 1 step: 36, loss is 2.8440022\n",
      "epoch: 1 step: 37, loss is 2.9362144\n",
      "epoch: 1 step: 38, loss is 3.0257726\n",
      "epoch: 1 step: 39, loss is 2.9223862\n",
      "epoch: 1 step: 40, loss is 2.7044935\n",
      "epoch: 1 step: 41, loss is 2.9694562\n",
      "epoch: 1 step: 42, loss is 3.0189013\n",
      "epoch: 1 step: 43, loss is 3.0601017\n",
      "epoch: 1 step: 44, loss is 2.9115818\n",
      "epoch: 1 step: 45, loss is 3.0184646\n",
      "epoch: 1 step: 46, loss is 2.975799\n",
      "epoch: 1 step: 47, loss is 2.8718266\n",
      "epoch: 1 step: 48, loss is 3.1396894\n",
      "epoch: 1 step: 49, loss is 2.950092\n",
      "epoch: 1 step: 50, loss is 2.7480369\n",
      "epoch: 1 step: 51, loss is 2.8395002\n",
      "epoch: 1 step: 52, loss is 3.1381648\n",
      "epoch: 1 step: 53, loss is 2.909213\n",
      "epoch: 1 step: 54, loss is 2.8059063\n",
      "epoch: 1 step: 55, loss is 2.726797\n",
      "epoch: 1 step: 56, loss is 2.938387\n",
      "epoch: 1 step: 57, loss is 2.8023553\n",
      "epoch: 1 step: 58, loss is 2.8491483\n",
      "epoch: 1 step: 59, loss is 2.9284897\n",
      "epoch: 1 step: 60, loss is 2.9622154\n",
      "epoch: 1 step: 61, loss is 2.7231977\n",
      "epoch: 1 step: 62, loss is 2.9902616\n",
      "epoch: 1 step: 63, loss is 2.927689\n",
      "epoch: 1 step: 64, loss is 2.9992876\n",
      "epoch: 1 step: 65, loss is 2.7974153\n",
      "epoch: 1 step: 66, loss is 2.9007182\n",
      "epoch: 1 step: 67, loss is 2.8791738\n",
      "epoch: 1 step: 68, loss is 2.7327795\n",
      "epoch: 1 step: 69, loss is 2.595337\n",
      "epoch: 1 step: 70, loss is 2.9379692\n",
      "epoch: 1 step: 71, loss is 3.0462499\n",
      "epoch: 1 step: 72, loss is 2.910287\n",
      "epoch: 1 step: 73, loss is 3.0235043\n",
      "epoch: 1 step: 74, loss is 2.9699833\n",
      "epoch: 1 step: 75, loss is 2.59862\n",
      "epoch: 1 step: 76, loss is 2.8716302\n",
      "epoch: 1 step: 77, loss is 3.0927196\n",
      "epoch: 1 step: 78, loss is 3.0082526\n",
      "epoch: 1 step: 79, loss is 3.0416179\n",
      "epoch: 1 step: 80, loss is 2.9235618\n",
      "epoch: 1 step: 81, loss is 3.1362908\n",
      "epoch: 1 step: 82, loss is 2.9299107\n",
      "epoch: 1 step: 83, loss is 2.7118602\n",
      "epoch: 1 step: 84, loss is 2.9725752\n",
      "epoch: 1 step: 85, loss is 2.843942\n",
      "epoch: 1 step: 86, loss is 3.114382\n",
      "epoch: 1 step: 87, loss is 2.967884\n",
      "epoch: 1 step: 88, loss is 2.9506643\n",
      "epoch: 1 step: 89, loss is 2.922656\n",
      "epoch: 1 step: 90, loss is 2.5929632\n",
      "epoch: 1 step: 91, loss is 2.8637176\n",
      "epoch: 1 step: 92, loss is 2.7643166\n",
      "epoch: 1 step: 93, loss is 3.1302218\n",
      "epoch: 1 step: 94, loss is 2.991366\n",
      "epoch: 1 step: 95, loss is 2.8120522\n",
      "epoch: 1 step: 96, loss is 2.69282\n",
      "epoch: 1 step: 97, loss is 2.8888388\n",
      "epoch: 1 step: 98, loss is 2.9651573\n",
      "epoch: 1 step: 99, loss is 2.8424087\n",
      "epoch: 1 step: 100, loss is 2.8959434\n",
      "epoch: 1 step: 101, loss is 3.0595188\n",
      "epoch: 1 step: 102, loss is 2.935608\n",
      "epoch: 1 step: 103, loss is 2.3676186\n",
      "epoch: 1 step: 104, loss is 3.1009436\n",
      "epoch: 1 step: 105, loss is 2.543276\n",
      "epoch: 1 step: 106, loss is 2.9885526\n",
      "epoch: 1 step: 107, loss is 3.0173173\n",
      "epoch: 1 step: 108, loss is 2.9682837\n",
      "epoch: 1 step: 109, loss is 2.8899288\n",
      "epoch: 1 step: 110, loss is 2.8917074\n",
      "epoch: 1 step: 111, loss is 2.9417136\n",
      "epoch: 1 step: 112, loss is 3.0589805\n",
      "epoch: 1 step: 113, loss is 2.7019782\n",
      "epoch: 1 step: 114, loss is 2.7574139\n",
      "epoch: 1 step: 115, loss is 3.0321472\n",
      "epoch: 1 step: 116, loss is 3.069345\n",
      "epoch: 1 step: 117, loss is 3.261804\n",
      "epoch: 1 step: 118, loss is 2.6414795\n",
      "epoch: 1 step: 119, loss is 3.1047692\n",
      "epoch: 1 step: 120, loss is 2.677041\n",
      "epoch: 1 step: 121, loss is 2.9569068\n",
      "epoch: 1 step: 122, loss is 3.1805134\n",
      "epoch: 1 step: 123, loss is 2.6630225\n",
      "epoch: 1 step: 124, loss is 2.9949691\n",
      "epoch: 1 step: 125, loss is 2.7086835\n",
      "epoch: 1 step: 126, loss is 2.9662633\n",
      "epoch: 1 step: 127, loss is 2.6560326\n",
      "epoch: 1 step: 128, loss is 2.6523619\n",
      "epoch: 1 step: 129, loss is 2.884564\n",
      "epoch: 1 step: 130, loss is 2.8614948\n",
      "epoch: 1 step: 131, loss is 3.115133\n",
      "epoch: 1 step: 132, loss is 3.0291004\n",
      "epoch: 1 step: 133, loss is 3.1113997\n",
      "epoch: 1 step: 134, loss is 2.7862086\n",
      "epoch: 1 step: 135, loss is 2.8854456\n",
      "epoch: 1 step: 136, loss is 2.733628\n",
      "epoch: 1 step: 137, loss is 3.0344563\n",
      "epoch: 1 step: 138, loss is 3.0565996\n",
      "epoch: 1 step: 139, loss is 3.186414\n",
      "epoch: 1 step: 140, loss is 3.0775573\n",
      "epoch: 1 step: 141, loss is 3.0126195\n",
      "epoch: 1 step: 142, loss is 2.9403846\n",
      "epoch: 1 step: 143, loss is 3.0715885\n",
      "epoch: 1 step: 144, loss is 3.1808937\n",
      "epoch: 1 step: 145, loss is 2.6877248\n",
      "epoch: 1 step: 146, loss is 2.9488444\n",
      "epoch: 1 step: 147, loss is 2.7599056\n",
      "epoch: 1 step: 148, loss is 3.0983877\n",
      "epoch: 1 step: 149, loss is 2.637927\n",
      "epoch: 1 step: 150, loss is 3.2293417\n",
      "epoch: 1 step: 151, loss is 2.704255\n",
      "epoch: 1 step: 152, loss is 2.940762\n",
      "epoch: 1 step: 153, loss is 3.0078123\n",
      "epoch: 1 step: 154, loss is 2.7532084\n",
      "epoch: 1 step: 155, loss is 2.7246435\n",
      "epoch: 1 step: 156, loss is 2.6197171\n",
      "epoch: 1 step: 157, loss is 3.0054598\n",
      "epoch: 1 step: 158, loss is 2.6150448\n",
      "epoch: 1 step: 159, loss is 2.9618676\n",
      "epoch: 1 step: 160, loss is 3.0246975\n",
      "epoch: 1 step: 161, loss is 2.8696234\n",
      "epoch: 1 step: 162, loss is 3.1050482\n",
      "epoch: 1 step: 163, loss is 2.9474983\n",
      "epoch: 1 step: 164, loss is 2.8221965\n",
      "epoch: 1 step: 165, loss is 2.9227488\n",
      "epoch: 1 step: 166, loss is 2.7446399\n",
      "epoch: 1 step: 167, loss is 3.1051352\n",
      "epoch: 1 step: 168, loss is 2.6006246\n",
      "epoch: 1 step: 169, loss is 2.770247\n",
      "epoch: 1 step: 170, loss is 2.809175\n",
      "epoch: 1 step: 171, loss is 2.9224458\n",
      "epoch: 1 step: 172, loss is 3.1823845\n",
      "epoch: 1 step: 173, loss is 2.8909159\n",
      "epoch: 1 step: 174, loss is 2.7560658\n",
      "epoch: 1 step: 175, loss is 2.738428\n",
      "epoch: 1 step: 176, loss is 2.7427955\n",
      "epoch: 1 step: 177, loss is 3.2622895\n",
      "epoch: 1 step: 178, loss is 2.5902557\n",
      "epoch: 1 step: 179, loss is 2.4553366\n",
      "epoch: 1 step: 180, loss is 2.8725162\n",
      "epoch: 1 step: 181, loss is 2.6774354\n",
      "epoch: 1 step: 182, loss is 2.8835292\n",
      "epoch: 1 step: 183, loss is 2.3791022\n",
      "epoch: 1 step: 184, loss is 2.438597\n",
      "epoch: 1 step: 185, loss is 2.482536\n",
      "epoch: 1 step: 186, loss is 2.4827757\n",
      "epoch: 1 step: 187, loss is 2.656946\n",
      "epoch: 1 step: 188, loss is 2.4318728\n",
      "epoch: 1 step: 189, loss is 2.7632413\n",
      "epoch: 1 step: 190, loss is 2.6837611\n",
      "epoch: 1 step: 191, loss is 2.5162668\n",
      "epoch: 1 step: 192, loss is 2.3779461\n",
      "epoch: 1 step: 193, loss is 2.4262338\n",
      "epoch: 1 step: 194, loss is 2.524579\n",
      "epoch: 1 step: 195, loss is 2.6191626\n",
      "epoch: 1 step: 196, loss is 2.7432916\n",
      "epoch: 1 step: 197, loss is 2.46064\n",
      "epoch: 1 step: 198, loss is 3.1478307\n",
      "epoch: 1 step: 199, loss is 2.1901202\n",
      "epoch: 1 step: 200, loss is 4.418375\n",
      "epoch: 1 step: 201, loss is 2.8642426\n",
      "epoch: 1 step: 202, loss is 2.5824513\n",
      "epoch: 1 step: 203, loss is 2.8141832\n",
      "epoch: 1 step: 204, loss is 3.7588515\n",
      "epoch: 1 step: 205, loss is 2.8955593\n",
      "epoch: 1 step: 206, loss is 3.266607\n",
      "epoch: 1 step: 207, loss is 3.159306\n",
      "epoch: 1 step: 208, loss is 3.1495748\n",
      "epoch: 1 step: 209, loss is 3.1089237\n",
      "epoch: 1 step: 210, loss is 3.1819263\n",
      "epoch: 1 step: 211, loss is 3.099749\n",
      "epoch: 1 step: 212, loss is 2.9675088\n",
      "epoch: 1 step: 213, loss is 3.079589\n",
      "epoch: 1 step: 214, loss is 2.8068137\n",
      "epoch: 1 step: 215, loss is 3.1694403\n",
      "epoch: 1 step: 216, loss is 2.860298\n",
      "epoch: 1 step: 217, loss is 3.0760155\n",
      "epoch: 1 step: 218, loss is 3.2263637\n",
      "epoch: 1 step: 219, loss is 3.0381556\n",
      "epoch: 1 step: 220, loss is 2.7734408\n",
      "epoch: 1 step: 221, loss is 2.7700844\n",
      "epoch: 1 step: 222, loss is 2.8694215\n",
      "epoch: 1 step: 223, loss is 2.707493\n",
      "epoch: 1 step: 224, loss is 2.8116732\n",
      "epoch: 1 step: 225, loss is 2.6717987\n",
      "epoch: 1 step: 226, loss is 2.6752176\n",
      "epoch: 1 step: 227, loss is 2.7012286\n",
      "epoch: 1 step: 228, loss is 2.4400406\n",
      "epoch: 1 step: 229, loss is 2.675255\n",
      "epoch: 1 step: 230, loss is 2.6405234\n",
      "epoch: 1 step: 231, loss is 2.5953312\n",
      "epoch: 1 step: 232, loss is 2.3846464\n",
      "epoch: 1 step: 233, loss is 2.4457436\n",
      "epoch: 1 step: 234, loss is 2.2202916\n",
      "epoch: 1 step: 235, loss is 2.696909\n",
      "epoch: 1 step: 236, loss is 2.9993722\n",
      "epoch: 1 step: 237, loss is 2.6094177\n",
      "epoch: 1 step: 238, loss is 2.3419933\n",
      "epoch: 1 step: 239, loss is 2.787599\n",
      "epoch: 1 step: 240, loss is 2.4425187\n",
      "epoch: 1 step: 241, loss is 2.347508\n",
      "epoch: 1 step: 242, loss is 2.9352608\n",
      "epoch: 1 step: 243, loss is 2.8394217\n",
      "epoch: 1 step: 244, loss is 2.5700126\n",
      "epoch: 1 step: 245, loss is 2.709556\n",
      "epoch: 1 step: 246, loss is 2.2570424\n",
      "epoch: 1 step: 247, loss is 2.5089083\n",
      "epoch: 1 step: 248, loss is 2.3397558\n",
      "epoch: 1 step: 249, loss is 2.383238\n",
      "epoch: 1 step: 250, loss is 2.4090183\n",
      "epoch: 1 step: 251, loss is 2.383617\n",
      "epoch: 1 step: 252, loss is 2.4288018\n",
      "epoch: 1 step: 253, loss is 2.1329784\n",
      "epoch: 1 step: 254, loss is 2.699294\n",
      "epoch: 1 step: 255, loss is 2.2192523\n",
      "epoch: 1 step: 256, loss is 2.4863\n",
      "epoch: 1 step: 257, loss is 2.3494425\n",
      "epoch: 1 step: 258, loss is 2.748976\n",
      "epoch: 1 step: 259, loss is 2.3552845\n",
      "epoch: 1 step: 260, loss is 2.4939268\n",
      "epoch: 1 step: 261, loss is 2.4509892\n",
      "epoch: 1 step: 262, loss is 2.393342\n",
      "epoch: 1 step: 263, loss is 1.9490278\n",
      "epoch: 1 step: 264, loss is 2.5468535\n",
      "epoch: 1 step: 265, loss is 2.6886742\n",
      "epoch: 1 step: 266, loss is 2.3863409\n",
      "epoch: 1 step: 267, loss is 2.059793\n",
      "epoch: 1 step: 268, loss is 2.2161345\n",
      "epoch: 1 step: 269, loss is 2.4038062\n",
      "epoch: 1 step: 270, loss is 2.2796617\n",
      "epoch: 1 step: 271, loss is 2.4172323\n",
      "epoch: 1 step: 272, loss is 2.4526765\n",
      "epoch: 1 step: 273, loss is 2.3918746\n",
      "epoch: 1 step: 274, loss is 2.003315\n",
      "epoch: 1 step: 275, loss is 2.2549677\n",
      "epoch: 1 step: 276, loss is 2.453614\n",
      "epoch: 1 step: 277, loss is 2.083204\n",
      "epoch: 1 step: 278, loss is 2.6255593\n",
      "epoch: 1 step: 279, loss is 2.0885885\n",
      "epoch: 1 step: 280, loss is 2.1577272\n",
      "epoch: 1 step: 281, loss is 2.2011209\n",
      "epoch: 1 step: 282, loss is 2.138446\n",
      "epoch: 1 step: 283, loss is 2.3677137\n",
      "epoch: 1 step: 284, loss is 2.1410718\n",
      "epoch: 1 step: 285, loss is 2.3953936\n",
      "epoch: 1 step: 286, loss is 2.3905346\n",
      "epoch: 1 step: 287, loss is 2.3880072\n",
      "epoch: 1 step: 288, loss is 2.1695724\n",
      "epoch: 1 step: 289, loss is 2.4393456\n",
      "epoch: 1 step: 290, loss is 2.2933288\n",
      "epoch: 1 step: 291, loss is 2.1674156\n",
      "epoch: 1 step: 292, loss is 1.9641832\n",
      "epoch: 1 step: 293, loss is 2.2768657\n",
      "epoch: 1 step: 294, loss is 2.502685\n",
      "epoch: 1 step: 295, loss is 2.2935555\n",
      "epoch: 1 step: 296, loss is 2.0192578\n",
      "epoch: 1 step: 297, loss is 2.145667\n",
      "epoch: 1 step: 298, loss is 2.4290628\n",
      "epoch: 1 step: 299, loss is 2.3616414\n",
      "epoch: 1 step: 300, loss is 2.3925216\n",
      "epoch: 1 step: 301, loss is 2.500598\n",
      "epoch: 1 step: 302, loss is 2.2805123\n",
      "epoch: 1 step: 303, loss is 2.5198553\n",
      "epoch: 1 step: 304, loss is 2.3642697\n",
      "epoch: 1 step: 305, loss is 1.9410217\n",
      "epoch: 1 step: 306, loss is 2.091194\n",
      "epoch: 1 step: 307, loss is 2.1131167\n",
      "epoch: 1 step: 308, loss is 2.5504713\n",
      "epoch: 1 step: 309, loss is 2.4046288\n",
      "epoch: 1 step: 310, loss is 2.0362718\n",
      "epoch: 1 step: 311, loss is 1.9420333\n",
      "epoch: 1 step: 312, loss is 2.5344281\n",
      "epoch: 1 step: 313, loss is 2.4814727\n",
      "epoch: 1 step: 314, loss is 2.1341736\n",
      "epoch: 1 step: 315, loss is 2.631365\n",
      "epoch: 1 step: 316, loss is 2.3015482\n",
      "epoch: 1 step: 317, loss is 2.3448226\n",
      "epoch: 1 step: 318, loss is 2.0173502\n",
      "epoch: 1 step: 319, loss is 2.254121\n",
      "epoch: 1 step: 320, loss is 2.1240726\n",
      "epoch: 1 step: 321, loss is 2.2633934\n",
      "epoch: 1 step: 322, loss is 2.133377\n",
      "epoch: 1 step: 323, loss is 2.2915096\n",
      "epoch: 1 step: 324, loss is 2.030336\n",
      "epoch: 1 step: 325, loss is 2.4794037\n",
      "epoch: 1 step: 326, loss is 2.2639897\n",
      "epoch: 1 step: 327, loss is 2.2449229\n",
      "epoch: 1 step: 328, loss is 2.2504363\n",
      "epoch: 1 step: 329, loss is 2.1988897\n",
      "epoch: 1 step: 330, loss is 2.4152553\n",
      "epoch: 1 step: 331, loss is 2.5051932\n",
      "epoch: 1 step: 332, loss is 2.3239741\n",
      "epoch: 1 step: 333, loss is 2.06885\n",
      "epoch: 1 step: 334, loss is 2.3222427\n",
      "epoch: 1 step: 335, loss is 2.3845227\n",
      "epoch: 1 step: 336, loss is 2.309745\n",
      "epoch: 1 step: 337, loss is 2.1951768\n",
      "epoch: 1 step: 338, loss is 2.2687688\n",
      "epoch: 1 step: 339, loss is 2.3154478\n",
      "epoch: 1 step: 340, loss is 2.1326718\n",
      "epoch: 1 step: 341, loss is 2.3594646\n",
      "epoch: 1 step: 342, loss is 2.3321147\n",
      "epoch: 1 step: 343, loss is 2.2741094\n",
      "epoch: 1 step: 344, loss is 2.137516\n",
      "epoch: 1 step: 345, loss is 2.021626\n",
      "epoch: 1 step: 346, loss is 1.8963099\n",
      "epoch: 1 step: 347, loss is 2.055245\n",
      "epoch: 1 step: 348, loss is 2.0979152\n",
      "epoch: 1 step: 349, loss is 2.1949575\n",
      "epoch: 1 step: 350, loss is 1.9552985\n",
      "epoch: 1 step: 351, loss is 2.117923\n",
      "epoch: 1 step: 352, loss is 2.2266371\n",
      "epoch: 1 step: 353, loss is 2.3477998\n",
      "epoch: 1 step: 354, loss is 2.3008065\n",
      "epoch: 1 step: 355, loss is 2.0902667\n",
      "epoch: 1 step: 356, loss is 2.231474\n",
      "epoch: 1 step: 357, loss is 1.9999826\n",
      "epoch: 1 step: 358, loss is 2.1942635\n",
      "epoch: 1 step: 359, loss is 2.1964118\n",
      "epoch: 1 step: 360, loss is 1.9054182\n",
      "epoch: 1 step: 361, loss is 2.3081508\n",
      "epoch: 1 step: 362, loss is 2.1026568\n",
      "epoch: 1 step: 363, loss is 2.1243508\n",
      "epoch: 1 step: 364, loss is 2.4789147\n",
      "epoch: 1 step: 365, loss is 2.275992\n",
      "epoch: 1 step: 366, loss is 2.203393\n",
      "epoch: 1 step: 367, loss is 2.036766\n",
      "epoch: 1 step: 368, loss is 2.4130359\n",
      "epoch: 1 step: 369, loss is 2.044419\n",
      "epoch: 1 step: 370, loss is 2.364053\n",
      "epoch: 1 step: 371, loss is 2.1048672\n",
      "epoch: 1 step: 372, loss is 2.1522758\n",
      "epoch: 1 step: 373, loss is 2.3075051\n",
      "epoch: 1 step: 374, loss is 2.2431762\n",
      "epoch: 1 step: 375, loss is 2.218287\n",
      "epoch: 1 step: 376, loss is 2.095737\n",
      "epoch: 1 step: 377, loss is 2.1652203\n",
      "epoch: 1 step: 378, loss is 2.0838752\n",
      "epoch: 1 step: 379, loss is 2.3840022\n",
      "epoch: 1 step: 380, loss is 2.0747614\n",
      "epoch: 1 step: 381, loss is 2.214645\n",
      "epoch: 1 step: 382, loss is 2.1585748\n",
      "epoch: 1 step: 383, loss is 2.5858536\n",
      "epoch: 1 step: 384, loss is 2.2462816\n",
      "epoch: 1 step: 385, loss is 2.0072455\n",
      "epoch: 1 step: 386, loss is 1.8495305\n",
      "epoch: 1 step: 387, loss is 2.2052016\n",
      "epoch: 1 step: 388, loss is 2.1939359\n",
      "epoch: 1 step: 389, loss is 1.9555017\n",
      "epoch: 1 step: 390, loss is 2.5153377\n",
      "epoch: 1 step: 391, loss is 2.1738896\n",
      "epoch: 1 step: 392, loss is 4.1913724\n",
      "epoch: 1 step: 393, loss is 2.3255944\n",
      "epoch: 1 step: 394, loss is 3.152139\n",
      "epoch: 1 step: 395, loss is 2.811295\n",
      "epoch: 1 step: 396, loss is 2.6550522\n",
      "epoch: 1 step: 397, loss is 2.6286287\n",
      "epoch: 1 step: 398, loss is 2.8930132\n",
      "epoch: 1 step: 399, loss is 2.3280838\n",
      "epoch: 1 step: 400, loss is 2.4574482\n",
      "epoch: 1 step: 401, loss is 2.2851586\n",
      "epoch: 1 step: 402, loss is 2.3333082\n",
      "epoch: 1 step: 403, loss is 2.1450624\n",
      "epoch: 1 step: 404, loss is 2.1369724\n",
      "epoch: 1 step: 405, loss is 2.2965767\n",
      "epoch: 1 step: 406, loss is 2.0560057\n",
      "epoch: 1 step: 407, loss is 2.5909505\n",
      "epoch: 1 step: 408, loss is 2.267137\n",
      "epoch: 1 step: 409, loss is 2.5706668\n",
      "epoch: 1 step: 410, loss is 2.0675583\n",
      "epoch: 1 step: 411, loss is 2.0135002\n",
      "epoch: 1 step: 412, loss is 2.3642073\n",
      "epoch: 1 step: 413, loss is 2.5038676\n",
      "epoch: 1 step: 414, loss is 2.3296442\n",
      "epoch: 1 step: 415, loss is 2.2940297\n",
      "epoch: 1 step: 416, loss is 1.9451909\n",
      "epoch: 1 step: 417, loss is 2.4034595\n",
      "epoch: 1 step: 418, loss is 1.8790994\n",
      "epoch: 1 step: 419, loss is 2.1419883\n",
      "epoch: 1 step: 420, loss is 2.3392704\n",
      "epoch: 1 step: 421, loss is 2.1641912\n",
      "epoch: 1 step: 422, loss is 2.3136506\n",
      "epoch: 1 step: 423, loss is 2.0889513\n",
      "epoch: 1 step: 424, loss is 2.1095266\n",
      "epoch: 1 step: 425, loss is 2.2789366\n",
      "epoch: 1 step: 426, loss is 2.3250656\n",
      "epoch: 1 step: 427, loss is 2.3552823\n",
      "epoch: 1 step: 428, loss is 2.0611982\n",
      "epoch: 1 step: 429, loss is 2.096585\n",
      "epoch: 1 step: 430, loss is 2.2127495\n",
      "epoch: 1 step: 431, loss is 2.1144388\n",
      "epoch: 1 step: 432, loss is 2.0731125\n",
      "epoch: 1 step: 433, loss is 2.0587478\n",
      "epoch: 1 step: 434, loss is 2.043555\n",
      "epoch: 1 step: 435, loss is 1.950695\n",
      "epoch: 1 step: 436, loss is 2.1858122\n",
      "epoch: 1 step: 437, loss is 2.2055354\n",
      "epoch: 1 step: 438, loss is 1.8139303\n",
      "epoch: 1 step: 439, loss is 2.389018\n",
      "epoch: 1 step: 440, loss is 2.2970927\n",
      "epoch: 1 step: 441, loss is 2.265742\n",
      "epoch: 1 step: 442, loss is 2.27355\n",
      "epoch: 1 step: 443, loss is 2.3455873\n",
      "epoch: 1 step: 444, loss is 2.0769725\n",
      "epoch: 1 step: 445, loss is 2.1221921\n",
      "epoch: 1 step: 446, loss is 2.1465168\n",
      "epoch: 1 step: 447, loss is 2.6333692\n",
      "epoch: 1 step: 448, loss is 2.0668263\n",
      "epoch: 1 step: 449, loss is 2.2056837\n",
      "epoch: 1 step: 450, loss is 2.2127135\n",
      "epoch: 1 step: 451, loss is 2.134716\n",
      "epoch: 1 step: 452, loss is 2.1482918\n",
      "epoch: 1 step: 453, loss is 2.4356132\n",
      "epoch: 1 step: 454, loss is 2.060442\n",
      "epoch: 1 step: 455, loss is 2.2013774\n",
      "epoch: 1 step: 456, loss is 2.1516032\n",
      "epoch: 1 step: 457, loss is 2.2656593\n",
      "epoch: 1 step: 458, loss is 2.3584332\n",
      "epoch: 1 step: 459, loss is 2.3622131\n",
      "epoch: 1 step: 460, loss is 2.038892\n",
      "epoch: 1 step: 461, loss is 2.3241506\n",
      "epoch: 1 step: 462, loss is 2.1642385\n",
      "epoch: 1 step: 463, loss is 2.3433404\n",
      "epoch: 1 step: 464, loss is 2.2151184\n",
      "epoch: 1 step: 465, loss is 2.2841115\n",
      "epoch: 1 step: 466, loss is 2.129043\n",
      "epoch: 1 step: 467, loss is 2.045056\n",
      "epoch: 1 step: 468, loss is 2.0255756\n",
      "epoch: 1 step: 469, loss is 2.1287012\n",
      "epoch: 1 step: 470, loss is 2.0058527\n",
      "epoch: 1 step: 471, loss is 2.0791817\n",
      "epoch: 1 step: 472, loss is 2.355754\n",
      "epoch: 1 step: 473, loss is 1.9175459\n",
      "epoch: 1 step: 474, loss is 2.1101599\n",
      "epoch: 1 step: 475, loss is 2.0840673\n",
      "epoch: 1 step: 476, loss is 2.1730242\n",
      "epoch: 1 step: 477, loss is 1.8939841\n",
      "epoch: 1 step: 478, loss is 2.2222629\n",
      "epoch: 1 step: 479, loss is 1.8475622\n",
      "epoch: 1 step: 480, loss is 2.1822953\n",
      "epoch: 1 step: 481, loss is 1.9296138\n",
      "epoch: 1 step: 482, loss is 2.0671902\n",
      "epoch: 1 step: 483, loss is 1.8685994\n",
      "epoch: 1 step: 484, loss is 2.2076006\n",
      "epoch: 1 step: 485, loss is 2.1664824\n",
      "epoch: 1 step: 486, loss is 2.1083584\n",
      "epoch: 1 step: 487, loss is 2.1526494\n",
      "epoch: 1 step: 488, loss is 1.7959454\n",
      "epoch: 1 step: 489, loss is 2.0649457\n",
      "epoch: 1 step: 490, loss is 1.9597721\n",
      "epoch: 1 step: 491, loss is 1.9243904\n",
      "epoch: 1 step: 492, loss is 2.1348534\n",
      "epoch: 1 step: 493, loss is 1.9645872\n",
      "epoch: 1 step: 494, loss is 2.0217407\n",
      "epoch: 1 step: 495, loss is 2.1375964\n",
      "epoch: 1 step: 496, loss is 2.0086603\n",
      "epoch: 1 step: 497, loss is 2.223705\n",
      "epoch: 1 step: 498, loss is 1.889609\n",
      "epoch: 1 step: 499, loss is 1.925564\n",
      "epoch: 1 step: 500, loss is 2.1169238\n",
      "epoch: 1 step: 501, loss is 2.122727\n",
      "epoch: 1 step: 502, loss is 2.0659258\n",
      "epoch: 1 step: 503, loss is 2.2351842\n",
      "epoch: 1 step: 504, loss is 1.983293\n",
      "epoch: 1 step: 505, loss is 2.097146\n",
      "epoch: 1 step: 506, loss is 1.9860979\n",
      "epoch: 1 step: 507, loss is 2.0768938\n",
      "epoch: 1 step: 508, loss is 2.214273\n",
      "epoch: 1 step: 509, loss is 2.1810174\n",
      "epoch: 1 step: 510, loss is 2.011674\n",
      "epoch: 1 step: 511, loss is 1.9866016\n",
      "epoch: 1 step: 512, loss is 2.0617454\n",
      "epoch: 1 step: 513, loss is 1.965301\n",
      "epoch: 1 step: 514, loss is 2.1814547\n",
      "epoch: 1 step: 515, loss is 2.2897182\n",
      "epoch: 1 step: 516, loss is 1.7619205\n",
      "epoch: 1 step: 517, loss is 1.8889265\n",
      "epoch: 1 step: 518, loss is 2.1875055\n",
      "epoch: 1 step: 519, loss is 2.3906083\n",
      "epoch: 1 step: 520, loss is 1.772217\n",
      "epoch: 1 step: 521, loss is 1.7471128\n",
      "epoch: 1 step: 522, loss is 2.012107\n",
      "epoch: 1 step: 523, loss is 2.142066\n",
      "epoch: 1 step: 524, loss is 1.7740906\n",
      "epoch: 1 step: 525, loss is 2.1380217\n",
      "epoch: 1 step: 526, loss is 1.9492394\n",
      "epoch: 1 step: 527, loss is 2.0925913\n",
      "epoch: 1 step: 528, loss is 2.4444\n",
      "epoch: 1 step: 529, loss is 1.9748669\n",
      "epoch: 1 step: 530, loss is 1.8960652\n",
      "epoch: 1 step: 531, loss is 1.9936131\n",
      "epoch: 1 step: 532, loss is 2.549995\n",
      "epoch: 1 step: 533, loss is 1.5654795\n",
      "epoch: 1 step: 534, loss is 1.9317423\n",
      "epoch: 1 step: 535, loss is 1.9313135\n",
      "epoch: 1 step: 536, loss is 2.2121606\n",
      "epoch: 1 step: 537, loss is 1.8132015\n",
      "epoch: 1 step: 538, loss is 1.837054\n",
      "epoch: 1 step: 539, loss is 1.8902675\n",
      "epoch: 1 step: 540, loss is 2.193503\n",
      "epoch: 1 step: 541, loss is 1.9481928\n",
      "epoch: 1 step: 542, loss is 1.9620379\n",
      "epoch: 1 step: 543, loss is 2.2436066\n",
      "epoch: 1 step: 544, loss is 2.285583\n",
      "epoch: 1 step: 545, loss is 1.7013383\n",
      "epoch: 1 step: 546, loss is 1.8303064\n",
      "epoch: 1 step: 547, loss is 2.026541\n",
      "epoch: 1 step: 548, loss is 1.746999\n",
      "epoch: 1 step: 549, loss is 1.6489731\n",
      "epoch: 1 step: 550, loss is 1.8849648\n",
      "epoch: 1 step: 551, loss is 2.124671\n",
      "epoch: 1 step: 552, loss is 1.9389162\n",
      "epoch: 1 step: 553, loss is 1.9396572\n",
      "epoch: 1 step: 554, loss is 1.8831288\n",
      "epoch: 1 step: 555, loss is 2.0134902\n",
      "epoch: 1 step: 556, loss is 1.8304744\n",
      "epoch: 1 step: 557, loss is 1.90921\n",
      "epoch: 1 step: 558, loss is 1.5246134\n",
      "epoch: 1 step: 559, loss is 1.7789774\n",
      "epoch: 1 step: 560, loss is 2.031227\n",
      "epoch: 1 step: 561, loss is 1.9113704\n",
      "epoch: 1 step: 562, loss is 1.9032419\n",
      "epoch: 1 step: 563, loss is 2.2706823\n",
      "epoch: 1 step: 564, loss is 2.3155854\n",
      "epoch: 1 step: 565, loss is 1.9243772\n",
      "epoch: 1 step: 566, loss is 2.0326097\n",
      "epoch: 1 step: 567, loss is 2.2155783\n",
      "epoch: 1 step: 568, loss is 2.1516929\n",
      "epoch: 1 step: 569, loss is 1.9595699\n",
      "epoch: 1 step: 570, loss is 1.862071\n",
      "epoch: 1 step: 571, loss is 1.9960926\n",
      "epoch: 1 step: 572, loss is 2.040969\n",
      "epoch: 1 step: 573, loss is 1.7542801\n",
      "epoch: 1 step: 574, loss is 1.9318788\n",
      "epoch: 1 step: 575, loss is 1.8586538\n",
      "epoch: 1 step: 576, loss is 1.9874213\n",
      "epoch: 1 step: 577, loss is 2.1999054\n",
      "epoch: 1 step: 578, loss is 2.1376095\n",
      "epoch: 1 step: 579, loss is 2.0808947\n",
      "epoch: 1 step: 580, loss is 1.8813667\n",
      "epoch: 1 step: 581, loss is 2.1261463\n",
      "epoch: 1 step: 582, loss is 1.9847945\n",
      "epoch: 1 step: 583, loss is 2.1443138\n",
      "epoch: 1 step: 584, loss is 2.1412277\n",
      "epoch: 1 step: 585, loss is 1.8111446\n",
      "epoch: 1 step: 586, loss is 2.379024\n",
      "epoch: 1 step: 587, loss is 1.9781998\n",
      "epoch: 1 step: 588, loss is 1.7665156\n",
      "epoch: 1 step: 589, loss is 2.0439572\n",
      "epoch: 1 step: 590, loss is 2.015919\n",
      "epoch: 1 step: 591, loss is 1.9242208\n",
      "epoch: 1 step: 592, loss is 1.9354713\n",
      "epoch: 1 step: 593, loss is 2.056019\n",
      "epoch: 1 step: 594, loss is 2.0855644\n",
      "epoch: 1 step: 595, loss is 1.7339765\n",
      "epoch: 1 step: 596, loss is 2.0726528\n",
      "epoch: 1 step: 597, loss is 1.9904325\n",
      "epoch: 1 step: 598, loss is 1.8439128\n",
      "epoch: 1 step: 599, loss is 2.017173\n",
      "epoch: 1 step: 600, loss is 1.7845192\n",
      "epoch: 1 step: 601, loss is 1.8566113\n",
      "epoch: 1 step: 602, loss is 1.8639197\n",
      "epoch: 1 step: 603, loss is 2.2160842\n",
      "epoch: 1 step: 604, loss is 1.6752913\n",
      "epoch: 1 step: 605, loss is 2.235425\n",
      "epoch: 1 step: 606, loss is 1.769102\n",
      "epoch: 1 step: 607, loss is 2.0450552\n",
      "epoch: 1 step: 608, loss is 1.8950784\n",
      "epoch: 1 step: 609, loss is 2.095818\n",
      "epoch: 1 step: 610, loss is 1.8324215\n",
      "epoch: 1 step: 611, loss is 1.8204062\n",
      "epoch: 1 step: 612, loss is 1.7785408\n",
      "epoch: 1 step: 613, loss is 1.8922622\n",
      "epoch: 1 step: 614, loss is 1.9366429\n",
      "epoch: 1 step: 615, loss is 1.9830577\n",
      "epoch: 1 step: 616, loss is 1.9977887\n",
      "epoch: 1 step: 617, loss is 2.0029593\n",
      "epoch: 1 step: 618, loss is 2.1935427\n",
      "epoch: 1 step: 619, loss is 2.040675\n",
      "epoch: 1 step: 620, loss is 1.8691549\n",
      "epoch: 1 step: 621, loss is 1.7655855\n",
      "epoch: 1 step: 622, loss is 2.0733917\n",
      "epoch: 1 step: 623, loss is 2.061674\n",
      "epoch: 1 step: 624, loss is 1.8825696\n",
      "epoch: 1 step: 625, loss is 1.9875604\n",
      "epoch: 1 step: 626, loss is 1.8188989\n",
      "epoch: 1 step: 627, loss is 1.9307184\n",
      "epoch: 1 step: 628, loss is 1.9086533\n",
      "epoch: 1 step: 629, loss is 2.2097929\n",
      "epoch: 1 step: 630, loss is 2.0483675\n",
      "epoch: 1 step: 631, loss is 1.9726732\n",
      "epoch: 1 step: 632, loss is 2.235858\n",
      "epoch: 1 step: 633, loss is 2.215693\n",
      "epoch: 1 step: 634, loss is 2.0745487\n",
      "epoch: 1 step: 635, loss is 1.919608\n",
      "epoch: 1 step: 636, loss is 1.9780576\n",
      "epoch: 1 step: 637, loss is 2.2710824\n",
      "epoch: 1 step: 638, loss is 1.9335372\n",
      "epoch: 1 step: 639, loss is 2.3248584\n",
      "epoch: 1 step: 640, loss is 2.0398314\n",
      "epoch: 1 step: 641, loss is 1.7193708\n",
      "epoch: 1 step: 642, loss is 2.1305897\n",
      "epoch: 1 step: 643, loss is 1.8780863\n",
      "epoch: 1 step: 644, loss is 1.7633858\n",
      "epoch: 1 step: 645, loss is 1.7441987\n",
      "epoch: 1 step: 646, loss is 1.9330364\n",
      "epoch: 1 step: 647, loss is 1.945266\n",
      "epoch: 1 step: 648, loss is 1.9891446\n",
      "epoch: 1 step: 649, loss is 1.9305825\n",
      "epoch: 1 step: 650, loss is 2.2957432\n",
      "epoch: 1 step: 651, loss is 1.8919885\n",
      "epoch: 1 step: 652, loss is 2.163204\n",
      "epoch: 1 step: 653, loss is 2.0149322\n",
      "epoch: 1 step: 654, loss is 2.051066\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class TrainOneStepCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    Network training package class.\n",
    "\n",
    "    Wraps the network with an optimizer. The resulting Cell be trained without inputs.\n",
    "    Backward graph will be created in the construct function to do parameter updating. Different\n",
    "    parallel modes are available to run the training.\n",
    "\n",
    "    Args:\n",
    "        network (Cell): The training network.\n",
    "        optimizer (Cell): Optimizer for updating the weights.\n",
    "        sens (Number): The scaling number to be filled as the input of backpropagation. Default value is 1.0.\n",
    "\n",
    "    Outputs:\n",
    "        Tensor, a scalar Tensor with shape :math:`()`.\n",
    "\n",
    "    Examples:\n",
    "        >>> net = Net()\n",
    "        >>> loss_fn = nn.SoftmaxCrossEntropyWithLogits()\n",
    "        >>> optim = nn.Momentum(net.trainable_params(), learning_rate=0.1, momentum=0.9)\n",
    "        >>> loss_net = nn.WithLossCell(net, loss_fn)\n",
    "        >>> train_net = nn.TrainOneStepCell(loss_net, optim)\n",
    "    \"\"\"\n",
    "    def __init__(self, network, optimizer, sens=1.0):\n",
    "        super(TrainOneStepCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.add_flags(defer_inline=True)\n",
    "        self.weights = ParameterTuple(network.trainable_params())\n",
    "        self.optimizer = optimizer\n",
    "        self.grad = C.GradOperation(get_by_list=True)\n",
    "        self.sens = sens\n",
    "\n",
    "    def construct(self, v, q, a, item, q_len):\n",
    "        weights = self.weights\n",
    "        loss = self.network(v, q, a, item, q_len)\n",
    "        sens = P.Fill()(P.DType()(loss), P.Shape()(loss), self.sens)\n",
    "        grads = self.grad(self.network, weights)(v, q, a, item, q_len)\n",
    "        return F.depend(loss, self.optimizer(grads))\n",
    "\n",
    "class NLLLoss(_Loss):\n",
    "    '''\n",
    "       NLLLoss function\n",
    "    '''\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(NLLLoss, self).__init__(reduction)\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "        self.log_softmax = P.LogSoftmax(axis=0)\n",
    "\n",
    "    def construct(self, logits, label):\n",
    "        nll = -self.log_softmax(logits)\n",
    "        loss = self.reduce_sum(nll * label / 10, axis=1).mean()\n",
    "        return self.get_loss(loss)\n",
    "\n",
    "class WithLossCell(nn.Cell):\n",
    "    \"\"\"\n",
    "    The cell wrapped with NLL loss, for train only\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone):\n",
    "        super(WithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._loss_fn = NLLLoss()\n",
    "        self._backbone = backbone\n",
    "        self.reduce_sum = P.ReduceSum()\n",
    "\n",
    "    def construct(self, v, q, a, item, q_len):\n",
    "        out = self._backbone(v, q, q_len)\n",
    "        loss = self._loss_fn(out, a)\n",
    "        return loss\n",
    "\n",
    "class TrainNetWrapper(nn.Cell):\n",
    "    def __init__(self, backbone):\n",
    "        super(TrainNetWrapper, self).__init__(auto_prefix=False)\n",
    "        self.net = backbone\n",
    "        \n",
    "        loss_net = WithLossCell(backbone)\n",
    "        optimizer = nn.Adam(params=net.trainable_params(), learning_rate=config.initial_lr)\n",
    "        \n",
    "        self.loss_train_net = TrainOneStepCell(loss_net, optimizer)\n",
    "\n",
    "    def construct(self, v, q, a, item, q_len):\n",
    "        loss = self.loss_train_net(v, q, a, item, q_len)\n",
    "        accuracy = Tensor(0.35)\n",
    "        return loss, accuracy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train now!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from datetime import datetime\n",
    "name = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "target_name = os.path.join('logs', '{}.ckpt'.format(name))\n",
    "print('will save to {}'.format(target_name))\n",
    "config_as_dict = {k: v for k, v in vars(config).items() if not k.startswith('__')}\n",
    "\n",
    "train_loader = data.get_loader(train=True)\n",
    "# val_loader = data.get_loader(val=True)\n",
    "\n",
    "net = model.Net(train_loader.source.num_tokens)\n",
    "if config.pretrained:\n",
    "    param_dict = load_checkpoint(config.pretrained_model_path)\n",
    "    if param_dict is not None: print(\"Successfully loaded pretrained model from {}.\".format(config.pretrained_model_path))\n",
    "    load_param_into_net(net, param_dict)\n",
    "\n",
    "tracker = utils.Tracker()\n",
    "train_net = TrainNetWrapper(net)\n",
    "step = 0\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    # train_loader = data.get_loader(train=True) # not sure if it matters?\n",
    "\n",
    "    \"\"\"\n",
    "    Hand-crafted train wiht `for` loop\n",
    "    \"\"\"\n",
    "    # train_net.set_train()\n",
    "    # for v, q, a, idx, q_len in train_loader:\n",
    "    #     train_result = train_net(v, q, a, idx, q_len)\n",
    "    #     train_loss = train_result[0]\n",
    "    #     train_acc = train_result[1]\n",
    "    #     print(\"T{} step {}: loss = {}, acc = {}\".format(epoch, step, train_loss, train_acc))\n",
    "    #     step += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    Wrapped train with `tqdm`\n",
    "    \"\"\"\n",
    "    run(train_net, train_loader, tracker, train=True, prefix='train', epoch=epoch)\n",
    "\n",
    "    # train_loader.reset() # not sure if it matters??\n",
    "    # break\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "will save to logs/2021-07-15_13:10:21.ckpt\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train E000: 164it [01:20,  2.05it/s, acc=0.3500, loss=3.6668]\n",
      "train E001: 164it [01:18,  2.08it/s, acc=0.3500, loss=3.0501]\n",
      "train E002: 138it [01:06,  2.07it/s, acc=0.3500, loss=2.9741]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1805932ba82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mWrapped\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \"\"\"\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# train_loader.reset() # not sure if it matters??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ef6e186c3502>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(net, loader, tracker, train, prefix, epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}_loss'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtracker_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0macc_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}_acc'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtracker_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/dataset/engine/iterators.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iterator does not have a running C++ pipeline.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mindspore/lib/python3.7/site-packages/mindspore/dataset/engine/iterators.py\u001b[0m in \u001b[0;36m_get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetNextAsList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def run(net, loader, tracker, train=False, prefix='', epoch=0):\n",
    "    \"\"\" Run an epoch over the given loader \"\"\"\n",
    "    if train:\n",
    "        net.set_train()\n",
    "        tracker_class, tracker_params = tracker.MovingMeanMonitor, {'momentum': 0.99}\n",
    "    else:\n",
    "        net.set_train(False)\n",
    "        tracker_class, tracker_params = tracker.MeanMonitor, {}\n",
    "\n",
    "    tq = tqdm(loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n",
    "    loss_tracker = tracker.track('{}_loss'.format(prefix), tracker_class(**tracker_params))\n",
    "    acc_tracker = tracker.track('{}_acc'.format(prefix), tracker_class(**tracker_params))\n",
    "    for v, q, a, idx, q_len in tq:\n",
    "        if train:\n",
    "            loss, acc = net(v, q, a, idx, q_len)\n",
    "        else:\n",
    "            print(\"Evaluating...\")\n",
    "        \n",
    "        loss_tracker.append(loss.asnumpy())\n",
    "        acc_tracker.append(acc.asnumpy())\n",
    "        # acc_tracker.append(acc.mean())\n",
    "        # for a in acc:\n",
    "        #     acc_tracker.append(a.item())\n",
    "        fmt = '{:.4f}'.format\n",
    "        tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}