{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the officially trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the pretrained model from [here](https://github.com/Cyanogenoid/pytorch-vqa/releases/tag/v1.0).\n",
    "\n",
    "Then move it to `logs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import math\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import data\n",
    "import model\n",
    "import utils\n",
    "\n",
    "\n",
    "def update_learning_rate(optimizer, iteration):\n",
    "    lr = config.initial_lr * 0.5**(float(iteration) / config.lr_halflife)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "\n",
    "def run(net, loader, optimizer, tracker, train=False, prefix='', epoch=0):\n",
    "    \"\"\" Run an epoch over the given loader \"\"\"\n",
    "    if train:\n",
    "        net.train()\n",
    "        tracker_class, tracker_params = tracker.MovingMeanMonitor, {'momentum': 0.99}\n",
    "    else:\n",
    "        net.eval()\n",
    "        tracker_class, tracker_params = tracker.MeanMonitor, {}\n",
    "        answ = []\n",
    "        idxs = []\n",
    "        accs = []\n",
    "\n",
    "    tq = tqdm(loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n",
    "    loss_tracker = tracker.track('{}_loss'.format(prefix), tracker_class(**tracker_params))\n",
    "    acc_tracker = tracker.track('{}_acc'.format(prefix), tracker_class(**tracker_params))\n",
    "\n",
    "    log_softmax = nn.LogSoftmax(dim=0).cuda()\n",
    "    for v, q, a, idx, q_len in tq:\n",
    "        var_params = {\n",
    "            # 'volatile': not train,\n",
    "            'requires_grad': False,\n",
    "        }\n",
    "        v = Variable(v.cuda(non_blocking=True), **var_params)\n",
    "        q = Variable(q.cuda(non_blocking=True), **var_params)\n",
    "        a = Variable(a.cuda(non_blocking=True), **var_params)\n",
    "        q_len = Variable(q_len.cuda(non_blocking=True), **var_params)\n",
    "\n",
    "        out = net(v, q, q_len)\n",
    "        nll = -log_softmax(out)\n",
    "        loss = (nll * a / 10).sum(dim=1).mean()\n",
    "        acc = utils.batch_accuracy(out.data, a.data).cpu()\n",
    "\n",
    "        if train:\n",
    "            global total_iterations\n",
    "            update_learning_rate(optimizer, total_iterations)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_iterations += 1\n",
    "        else:\n",
    "            # store information about evaluation of this minibatch\n",
    "            _, answer = out.data.cpu().max(dim=1)\n",
    "            answ.append(answer.view(-1))\n",
    "            accs.append(acc.view(-1))\n",
    "            idxs.append(idx.view(-1).clone())\n",
    "        \n",
    "        loss_tracker.append(loss.item())\n",
    "        # acc_tracker.append(acc.mean())\n",
    "        for a in acc:\n",
    "            acc_tracker.append(a.item())\n",
    "        fmt = '{:.4f}'.format\n",
    "        tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))\n",
    "\n",
    "    if not train:\n",
    "        answ = list(torch.cat(answ, dim=0))\n",
    "        accs = list(torch.cat(accs, dim=0))\n",
    "        idxs = list(torch.cat(idxs, dim=0))\n",
    "        return answ, accs, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will save to logs/2021-07-10_23:20:00.pth\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "name = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "target_name = os.path.join('logs', '{}.pth'.format(name))\n",
    "print('will save to {}'.format(target_name))\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "train_loader = data.get_loader(train=True)\n",
    "val_loader = data.get_loader(val=True)\n",
    "\n",
    "# net = nn.DataParallel(model.Net(train_loader.dataset.num_tokens)).cuda()\n",
    "# inner_net = net.module\n",
    "\n",
    "# Load the official pretrained model\n",
    "log = torch.load('logs/2017-08-04_00.55.19.pth')\n",
    "tokens = len(log['vocab']['question']) + 1\n",
    "net = nn.DataParallel(model.Net(tokens)).cuda()\n",
    "inner_net = net.module\n",
    "inner_net.classifier = model.Classifier(\n",
    "    in_features=2 * config.output_features + 1024,\n",
    "    mid_features=1024,\n",
    "    out_features=3000, # replaced for loading\n",
    "    drop=0.5,\n",
    ")\n",
    "\n",
    "net.load_state_dict(log['weights']) # load!\n",
    "\n",
    "# No need autograd\n",
    "for param in inner_net.parameters():\n",
    "    inner_net.requires_grad = False\n",
    "# Replace the textprocessor layer\n",
    "inner_net.text = model.TextProcessor(\n",
    "    embedding_tokens=train_loader.dataset.num_tokens,\n",
    "    embedding_features=300,\n",
    "    lstm_features=1024,\n",
    "    drop=0.5,\n",
    ")\n",
    "inner_net.text.cuda()\n",
    "# Replace the classifier layer\n",
    "inner_net.classifier = model.Classifier(\n",
    "    in_features=2 * config.output_features + 1024,\n",
    "    mid_features=1024,\n",
    "    out_features=config.max_answers,\n",
    "    drop=0.5,\n",
    ")\n",
    "inner_net.classifier.cuda()\n",
    "\n",
    "optimizer = optim.Adam([p for p in net.parameters() if p.requires_grad])\n",
    "\n",
    "tracker = utils.Tracker()\n",
    "config_as_dict = {k: v for k, v in vars(config).items() if not k.startswith('__')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train E000: 100% 164/164 [00:27<00:00,  5.95it/s, acc=0.2698, loss=3.4048]\n",
      "val E000: 100% 168/168 [00:20<00:00,  8.11it/s, acc=0.2690, loss=2.6996]\n",
      "train E001: 100% 164/164 [00:22<00:00,  7.38it/s, acc=0.2072, loss=2.9109]\n",
      "val E001: 100% 168/168 [00:21<00:00,  7.89it/s, acc=0.1124, loss=2.5225]\n",
      "train E002: 100% 164/164 [00:22<00:00,  7.25it/s, acc=0.3612, loss=2.7491]\n",
      "val E002: 100% 168/168 [00:19<00:00,  8.43it/s, acc=0.3152, loss=2.4384]\n",
      "train E003: 100% 164/164 [00:20<00:00,  8.01it/s, acc=0.4153, loss=2.6307]\n",
      "val E003: 100% 168/168 [00:21<00:00,  7.83it/s, acc=0.3268, loss=2.3994]\n",
      "train E004: 100% 164/164 [00:21<00:00,  7.70it/s, acc=0.4409, loss=2.6010]\n",
      "val E004: 100% 168/168 [00:19<00:00,  8.51it/s, acc=0.3278, loss=2.3864]\n",
      "train E005: 100% 164/164 [00:22<00:00,  7.43it/s, acc=0.4285, loss=2.5653]\n",
      "val E005: 100% 168/168 [00:21<00:00,  7.84it/s, acc=0.3299, loss=2.3693]\n",
      "train E006: 100% 164/164 [00:22<00:00,  7.26it/s, acc=0.4922, loss=2.5296]\n",
      "val E006: 100% 168/168 [00:21<00:00,  7.96it/s, acc=0.3358, loss=2.3507]\n",
      "train E007: 100% 164/164 [00:19<00:00,  8.34it/s, acc=0.3934, loss=2.4232]\n",
      "val E007: 100% 168/168 [00:20<00:00,  8.37it/s, acc=0.3462, loss=2.3569]\n",
      "train E008: 100% 164/164 [00:19<00:00,  8.26it/s, acc=0.4912, loss=2.4363]\n",
      "val E008: 100% 168/168 [00:21<00:00,  7.75it/s, acc=0.3552, loss=2.3512]\n",
      "train E009: 100% 164/164 [00:22<00:00,  7.42it/s, acc=0.5331, loss=2.3794]\n",
      "val E009: 100% 168/168 [00:21<00:00,  7.93it/s, acc=0.3589, loss=2.3506]\n",
      "train E010: 100% 164/164 [00:21<00:00,  7.46it/s, acc=0.4938, loss=2.3188]\n",
      "val E010: 100% 168/168 [00:21<00:00,  7.73it/s, acc=0.3507, loss=2.3414]\n",
      "train E011: 100% 164/164 [00:21<00:00,  7.47it/s, acc=0.4814, loss=2.3440]\n",
      "val E011: 100% 168/168 [00:20<00:00,  8.33it/s, acc=0.3648, loss=2.3458]\n",
      "train E012: 100% 164/164 [00:20<00:00,  8.01it/s, acc=0.5362, loss=2.3213]\n",
      "val E012: 100% 168/168 [00:19<00:00,  8.55it/s, acc=0.3583, loss=2.3646]\n",
      "train E013: 100% 164/164 [00:20<00:00,  7.88it/s, acc=0.5132, loss=2.2571]\n",
      "val E013: 100% 168/168 [00:20<00:00,  8.06it/s, acc=0.3636, loss=2.3563]\n",
      "train E014: 100% 164/164 [00:20<00:00,  7.89it/s, acc=0.5904, loss=2.2835]\n",
      "val E014: 100% 168/168 [00:19<00:00,  8.61it/s, acc=0.3560, loss=2.3685]\n",
      "train E015: 100% 164/164 [00:22<00:00,  7.24it/s, acc=0.5614, loss=2.2824]\n",
      "val E015: 100% 168/168 [00:21<00:00,  7.80it/s, acc=0.3676, loss=2.3530]\n",
      "train E016: 100% 164/164 [00:21<00:00,  7.54it/s, acc=0.5375, loss=2.2207]\n",
      "val E016: 100% 168/168 [00:20<00:00,  8.37it/s, acc=0.3744, loss=2.3548]\n",
      "train E017: 100% 164/164 [00:21<00:00,  7.71it/s, acc=0.5809, loss=2.2809]\n",
      "val E017: 100% 168/168 [00:21<00:00,  7.78it/s, acc=0.3740, loss=2.3689]\n",
      "train E018: 100% 164/164 [00:20<00:00,  7.82it/s, acc=0.5540, loss=2.2340]\n",
      "val E018: 100% 168/168 [00:19<00:00,  8.79it/s, acc=0.3821, loss=2.3685]\n",
      "train E019: 100% 164/164 [00:20<00:00,  7.83it/s, acc=0.6042, loss=2.2226]\n",
      "val E019: 100% 168/168 [00:20<00:00,  8.37it/s, acc=0.3845, loss=2.3741]\n",
      "train E020: 100% 164/164 [00:20<00:00,  7.96it/s, acc=0.5806, loss=2.1794]\n",
      "val E020: 100% 168/168 [00:21<00:00,  7.67it/s, acc=0.3790, loss=2.3718]\n",
      "train E021: 100% 164/164 [00:22<00:00,  7.28it/s, acc=0.5808, loss=2.1582]\n",
      "val E021: 100% 168/168 [00:21<00:00,  7.96it/s, acc=0.3894, loss=2.3671]\n",
      "train E022: 100% 164/164 [00:21<00:00,  7.46it/s, acc=0.5847, loss=2.1245]\n",
      "val E022: 100% 168/168 [00:19<00:00,  8.66it/s, acc=0.3870, loss=2.3890]\n",
      "train E023: 100% 164/164 [00:22<00:00,  7.37it/s, acc=0.5960, loss=2.2142]\n",
      "val E023: 100% 168/168 [00:19<00:00,  8.44it/s, acc=0.3755, loss=2.3770]\n",
      "train E024: 100% 164/164 [00:21<00:00,  7.66it/s, acc=0.5884, loss=2.1501]\n",
      "val E024: 100% 168/168 [00:19<00:00,  8.70it/s, acc=0.3845, loss=2.3913]\n",
      "train E025: 100% 164/164 [00:21<00:00,  7.53it/s, acc=0.5819, loss=2.0851]\n",
      "val E025: 100% 168/168 [00:20<00:00,  8.36it/s, acc=0.3822, loss=2.4072]\n",
      "train E026: 100% 164/164 [00:19<00:00,  8.22it/s, acc=0.6362, loss=2.1466]\n",
      "val E026: 100% 168/168 [00:19<00:00,  8.42it/s, acc=0.3803, loss=2.4341]\n",
      "train E027: 100% 164/164 [00:20<00:00,  8.07it/s, acc=0.5686, loss=2.0707]\n",
      "val E027: 100% 168/168 [00:19<00:00,  8.48it/s, acc=0.3808, loss=2.4051]\n",
      "train E028: 100% 164/164 [00:22<00:00,  7.29it/s, acc=0.6115, loss=2.0902]\n",
      "val E028: 100% 168/168 [00:21<00:00,  7.87it/s, acc=0.3870, loss=2.3796]\n",
      "train E029: 100% 164/164 [00:20<00:00,  8.01it/s, acc=0.6553, loss=2.1417]\n",
      "val E029: 100% 168/168 [00:19<00:00,  8.48it/s, acc=0.3851, loss=2.4109]\n",
      "train E030: 100% 164/164 [00:20<00:00,  8.07it/s, acc=0.7154, loss=2.0715]\n",
      "val E030: 100% 168/168 [00:19<00:00,  8.54it/s, acc=0.3891, loss=2.3947]\n",
      "train E031: 100% 164/164 [00:22<00:00,  7.43it/s, acc=0.5832, loss=2.1439]\n",
      "val E031: 100% 168/168 [00:21<00:00,  7.71it/s, acc=0.3785, loss=2.4091]\n",
      "train E032: 100% 164/164 [00:21<00:00,  7.49it/s, acc=0.6909, loss=2.1593]\n",
      "val E032: 100% 168/168 [00:19<00:00,  8.46it/s, acc=0.3808, loss=2.4293]\n",
      "train E033: 100% 164/164 [00:22<00:00,  7.38it/s, acc=0.6434, loss=2.1154]\n",
      "val E033: 100% 168/168 [00:21<00:00,  7.94it/s, acc=0.3879, loss=2.4194]\n",
      "train E034: 100% 164/164 [00:19<00:00,  8.28it/s, acc=0.6171, loss=2.0566]\n",
      "val E034: 100% 168/168 [00:21<00:00,  7.78it/s, acc=0.3849, loss=2.4340]\n",
      "train E035: 100% 164/164 [00:21<00:00,  7.47it/s, acc=0.7212, loss=2.0608]\n",
      "val E035: 100% 168/168 [00:19<00:00,  8.57it/s, acc=0.3825, loss=2.4527]\n",
      "train E036: 100% 164/164 [00:22<00:00,  7.45it/s, acc=0.7020, loss=2.1042]\n",
      "val E036: 100% 168/168 [00:21<00:00,  7.65it/s, acc=0.3833, loss=2.4271]\n",
      "train E037: 100% 164/164 [00:20<00:00,  7.91it/s, acc=0.7021, loss=2.0302]\n",
      "val E037: 100% 168/168 [00:20<00:00,  8.37it/s, acc=0.3862, loss=2.4520]\n",
      "train E038: 100% 164/164 [00:21<00:00,  7.55it/s, acc=0.7010, loss=2.0604]\n",
      "val E038: 100% 168/168 [00:22<00:00,  7.63it/s, acc=0.3841, loss=2.4527]\n",
      "train E039: 100% 164/164 [00:22<00:00,  7.15it/s, acc=0.6772, loss=1.9973]\n",
      "val E039: 100% 168/168 [00:19<00:00,  8.52it/s, acc=0.3846, loss=2.4654]\n",
      "train E040: 100% 164/164 [00:20<00:00,  8.19it/s, acc=0.7059, loss=2.1069]\n",
      "val E040: 100% 168/168 [00:20<00:00,  8.19it/s, acc=0.3826, loss=2.4712]\n",
      "train E041: 100% 164/164 [00:22<00:00,  7.31it/s, acc=0.7059, loss=1.9929]\n",
      "val E041: 100% 168/168 [00:20<00:00,  8.31it/s, acc=0.3810, loss=2.4738]\n",
      "train E042: 100% 164/164 [00:22<00:00,  7.40it/s, acc=0.7250, loss=2.0201]\n",
      "val E042: 100% 168/168 [00:21<00:00,  7.96it/s, acc=0.3852, loss=2.4741]\n",
      "train E043: 100% 164/164 [00:20<00:00,  8.18it/s, acc=0.6795, loss=2.0202]\n",
      "val E043: 100% 168/168 [00:19<00:00,  8.65it/s, acc=0.3903, loss=2.4503]\n",
      "train E044: 100% 164/164 [00:21<00:00,  7.47it/s, acc=0.7357, loss=2.0358]\n",
      "val E044: 100% 168/168 [00:21<00:00,  7.77it/s, acc=0.3886, loss=2.4710]\n",
      "train E045: 100% 164/164 [00:20<00:00,  7.83it/s, acc=0.7211, loss=2.0350]\n",
      "val E045: 100% 168/168 [00:21<00:00,  7.64it/s, acc=0.3889, loss=2.4711]\n",
      "train E046: 100% 164/164 [00:19<00:00,  8.26it/s, acc=0.7245, loss=2.0180]\n",
      "val E046: 100% 168/168 [00:20<00:00,  8.19it/s, acc=0.3893, loss=2.4671]\n",
      "train E047: 100% 164/164 [00:19<00:00,  8.29it/s, acc=0.7455, loss=2.0426]\n",
      "val E047: 100% 168/168 [00:20<00:00,  8.40it/s, acc=0.3907, loss=2.4975]\n",
      "train E048: 100% 164/164 [00:20<00:00,  8.07it/s, acc=0.6980, loss=2.0062]\n",
      "val E048: 100% 168/168 [00:19<00:00,  8.60it/s, acc=0.3899, loss=2.4906]\n",
      "train E049: 100% 164/164 [00:20<00:00,  8.06it/s, acc=0.6537, loss=2.0091]\n",
      "val E049: 100% 168/168 [00:19<00:00,  8.51it/s, acc=0.3918, loss=2.5083]\n"
     ]
    }
   ],
   "source": [
    "for i in range(config.epochs):\n",
    "    _ = run(net, train_loader, optimizer, tracker, train=True, prefix='train', epoch=i)\n",
    "    r = run(net, val_loader, optimizer, tracker, train=False, prefix='val', epoch=i)\n",
    "\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'tracker': tracker.to_dict(),\n",
    "        'config': config_as_dict,\n",
    "        'weights': net.state_dict(),\n",
    "        'eval': {\n",
    "            'answers': r[0],\n",
    "            'accuracies': r[1],\n",
    "            'idx': r[2],\n",
    "        },\n",
    "        'vocab': train_loader.dataset.vocab,\n",
    "    }\n",
    "    torch.save(results, target_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b88593a54178c4ad0f6717b1d9b69bdf93074f862ea03a9bcbac6eb406e2a88"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('AI': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}